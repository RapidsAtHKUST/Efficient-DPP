//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19867135
// Driver 352.39
// Based on LLVM 3.4svn
//

.version 4.3
.target sm_35, texmode_independent
.address_size 64

	// .globl	map_trans
.const .align 8 .b8 __internal_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.entry map_trans(
	.param .u64 .ptr .global .align 8 map_trans_param_0,
	.param .u64 .ptr .global .align 8 map_trans_param_1,
	.param .f64 map_trans_param_2,
	.param .u64 .ptr .global .align 8 map_trans_param_3,
	.param .u64 .ptr .global .align 8 map_trans_param_4,
	.param .u64 .ptr .global .align 8 map_trans_param_5,
	.param .u32 map_trans_param_6
)
{
	.local .align 8 .b8 	__local_depot0[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .b32 	%r<213>;
	.reg .f64 	%fd<340>;
	.reg .b64 	%rd<426>;


	mov.u64 	%rd425, __local_depot0;
	cvta.local.u64 	%SP, %rd425;
	ld.param.f64 	%fd43, [map_trans_param_2];
	ld.param.u64 	%rd182, [map_trans_param_3];
	ld.param.u64 	%rd183, [map_trans_param_4];
	ld.param.u64 	%rd184, [map_trans_param_5];
	ld.param.u32 	%r68, [map_trans_param_6];
	add.u64 	%rd185, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd185;
	setp.lt.s32	%p1, %r68, 1;
	@%p1 bra 	BB0_81;

	mov.u32 	%r70, %ctaid.x;
	mov.u32 	%r71, %ntid.x;
	mov.b32	%r72, %envreg3;
	mad.lo.s32 	%r73, %r70, %r71, %r72;
	mov.u32 	%r74, %tid.x;
	add.s32 	%r75, %r73, %r74;
	shr.s32 	%r76, %r75, 31;
	shr.u32 	%r77, %r76, 28;
	add.s32 	%r78, %r75, %r77;
	shr.s32 	%r79, %r78, 4;
	shl.b32 	%r80, %r68, 4;
	and.b32  	%r81, %r75, 15;
	mad.lo.s32 	%r184, %r80, %r79, %r81;
	add.s64 	%rd6, %rd1, 8;
	mov.u32 	%r183, 0;

BB0_2:
	ld.param.u64 	%rd364, [map_trans_param_1];
	cvt.s64.s32	%rd7, %r184;
	mul.wide.s32 	%rd186, %r184, 8;
	add.s64 	%rd187, %rd364, %rd186;
	ld.global.f64 	%fd1, [%rd187];
	abs.f64 	%fd2, %fd1;
	setp.eq.f64	%p2, %fd2, 0d7FF0000000000000;
	mov.f64 	%fd44, 0dFFF8000000000000;
	mov.f64 	%fd333, %fd44;
	@%p2 bra 	BB0_21;

	abs.f64 	%fd326, %fd1;
	setp.gt.f64	%p3, %fd326, 0d41E0000000000000;
	@%p3 bra 	BB0_5;
	bra.uni 	BB0_4;

BB0_5:
	mov.u64 	%rd368, %rd1;
	mov.b64 	 %rd9, %fd1;
	and.b64  	%rd10, %rd9, -9223372036854775808;
	shr.u64 	%rd11, %rd9, 52;
	bfe.u64 	%rd189, %rd9, 52, 11;
	add.s64 	%rd190, %rd189, 4294966272;
	cvt.u32.u64	%r5, %rd190;
	shr.u32 	%r83, %r5, 6;
	mov.u32 	%r84, 16;
	sub.s32 	%r6, %r84, %r83;
	mov.u32 	%r85, 19;
	sub.s32 	%r86, %r85, %r83;
	mov.u32 	%r87, 18;
	min.s32 	%r7, %r87, %r86;
	setp.gt.s32	%p4, %r6, %r7;
	mov.u64 	%rd369, 0;
	@%p4 bra 	BB0_8;

	shl.b64 	%rd192, %rd9, 11;
	or.b64  	%rd12, %rd192, -9223372036854775808;
	add.s32 	%r8, %r6, -1;
	mov.u64 	%rd367, %rd1;
	cvt.u32.u64	%r88, %rd11;
	and.b32  	%r89, %r88, 2047;
	add.s32 	%r90, %r89, -1024;
	shr.u32 	%r91, %r90, 6;
	neg.s32 	%r92, %r91;
	mul.wide.s32 	%rd194, %r92, 8;
	mov.u64 	%rd195, __internal_i2opi_d;
	add.s64 	%rd196, %rd195, %rd194;
	add.s64 	%rd366, %rd196, 120;
	mov.u64 	%rd369, 0;
	mov.u32 	%r185, %r8;
	mov.u64 	%rd411, %rd1;

BB0_7:
	.pragma "nounroll";
	mov.u64 	%rd15, %rd411;
	mov.u32 	%r9, %r185;
	ld.const.u64 	%rd197, [%rd366];
	mul.lo.s64 	%rd198, %rd197, %rd12;
	mul.hi.u64 	%rd199, %rd197, %rd12;
	add.s64 	%rd200, %rd369, %rd198;
	setp.lt.u64	%p5, %rd200, %rd369;
	selp.u64	%rd201, 1, 0, %p5;
	add.s64 	%rd369, %rd201, %rd199;
	st.local.u64 	[%rd367], %rd200;
	add.s32 	%r10, %r9, 1;
	sub.s32 	%r93, %r10, %r8;
	mul.wide.s32 	%rd202, %r93, 8;
	add.s64 	%rd367, %rd1, %rd202;
	add.s64 	%rd366, %rd366, 8;
	setp.lt.s32	%p6, %r10, %r7;
	add.s64 	%rd23, %rd15, 8;
	mov.u64 	%rd368, %rd23;
	mov.u32 	%r185, %r10;
	mov.u64 	%rd411, %rd23;
	@%p6 bra 	BB0_7;

BB0_8:
	st.local.u64 	[%rd368], %rd369;
	ld.local.u64 	%rd370, [%rd1+24];
	ld.local.u64 	%rd371, [%rd1+16];
	and.b32  	%r94, %r5, 63;
	setp.eq.s32	%p7, %r94, 0;
	@%p7 bra 	BB0_10;

	cvt.u32.u64	%r95, %rd11;
	and.b32  	%r96, %r95, 63;
	shl.b64 	%rd203, %rd370, %r96;
	neg.s64 	%rd204, %rd11;
	cvt.u32.u64	%r97, %rd204;
	and.b32  	%r98, %r97, 63;
	shr.u64 	%rd205, %rd371, %r98;
	or.b64  	%rd370, %rd205, %rd203;
	shl.b64 	%rd206, %rd371, %r96;
	ld.local.u64 	%rd207, [%rd6];
	shr.u64 	%rd208, %rd207, %r98;
	or.b64  	%rd371, %rd208, %rd206;

BB0_10:
	shr.u64 	%rd209, %rd370, 62;
	cvt.u32.u64	%r99, %rd209;
	shr.u64 	%rd210, %rd371, 62;
	shl.b64 	%rd211, %rd370, 2;
	or.b64  	%rd376, %rd210, %rd211;
	shl.b64 	%rd33, %rd371, 2;
	setp.ne.s64	%p8, %rd33, 0;
	selp.u64	%rd212, 1, 0, %p8;
	or.b64  	%rd213, %rd212, %rd376;
	setp.gt.u64	%p9, %rd213, -9223372036854775808;
	selp.u32	%r100, 1, 0, %p9;
	add.s32 	%r11, %r100, %r99;
	setp.lt.u64	%p10, %rd213, -9223372036854775807;
	mov.u64 	%rd372, %rd10;
	mov.u64 	%rd375, %rd33;
	@%p10 bra 	BB0_12;

	not.b64 	%rd214, %rd376;
	neg.s64 	%rd34, %rd33;
	setp.eq.s64	%p11, %rd33, 0;
	selp.u64	%rd215, 1, 0, %p11;
	add.s64 	%rd376, %rd215, %rd214;
	xor.b64  	%rd36, %rd10, -9223372036854775808;
	mov.u64 	%rd372, %rd36;
	mov.u64 	%rd375, %rd34;

BB0_12:
	mov.u64 	%rd374, %rd375;
	mov.u64 	%rd38, %rd372;
	neg.s32 	%r103, %r11;
	setp.eq.s64	%p12, %rd10, 0;
	selp.b32	%r191, %r11, %r103, %p12;
	mov.u32 	%r189, 0;
	mov.u32 	%r190, %r189;
	setp.lt.s64	%p13, %rd376, 1;
	@%p13 bra 	BB0_14;

BB0_13:
	shr.u64 	%rd216, %rd374, 63;
	shl.b64 	%rd217, %rd376, 1;
	or.b64  	%rd376, %rd216, %rd217;
	shl.b64 	%rd374, %rd374, 1;
	add.s32 	%r190, %r190, -1;
	setp.gt.s64	%p14, %rd376, 0;
	mov.u32 	%r189, %r190;
	@%p14 bra 	BB0_13;

BB0_14:
	mov.u32 	%r188, %r189;
	mul.lo.s64 	%rd378, %rd376, -3958705157555305931;
	mov.u64 	%rd218, -3958705157555305931;
	mul.hi.u64 	%rd377, %rd376, %rd218;
	setp.lt.s64	%p15, %rd377, 1;
	@%p15 bra 	BB0_16;

	shl.b64 	%rd219, %rd377, 1;
	shr.u64 	%rd220, %rd378, 63;
	or.b64  	%rd377, %rd219, %rd220;
	mul.lo.s64 	%rd378, %rd376, -7917410315110611862;
	add.s32 	%r188, %r188, -1;

BB0_16:
	setp.ne.s64	%p16, %rd378, 0;
	selp.u64	%rd221, 1, 0, %p16;
	add.s64 	%rd222, %rd221, %rd377;
	add.s32 	%r104, %r188, 1022;
	cvt.u64.u32	%rd223, %r104;
	shl.b64 	%rd224, %rd223, 52;
	shr.u64 	%rd225, %rd222, 11;
	add.s64 	%rd226, %rd224, %rd225;
	bfe.u64 	%rd227, %rd222, 10, 1;
	add.s64 	%rd228, %rd226, %rd227;
	or.b64  	%rd229, %rd228, %rd38;
	mov.b64 	 %fd327, %rd229;
	bra.uni 	BB0_17;

BB0_4:
	mov.f64 	%fd58, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd45, %fd1, %fd58;
	// inline asm
	cvt.rni.s32.f64 	%r191, %fd45;
	// inline asm
	cvt.rn.f64.s32	%fd59, %r191;
	neg.f64 	%fd55, %fd59;
	mov.f64 	%fd48, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd46, %fd55, %fd48, %fd1;
	// inline asm
	mov.f64 	%fd52, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd50, %fd55, %fd52, %fd46;
	// inline asm
	mov.f64 	%fd56, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd327, %fd55, %fd56, %fd50;
	// inline asm

BB0_17:
	add.s32 	%r19, %r191, 1;
	and.b32  	%r105, %r19, 1;
	setp.eq.b32	%p17, %r105, 1;
	mul.rn.f64 	%fd6, %fd327, %fd327;
	@!%p17 bra 	BB0_19;
	bra.uni 	BB0_18;

BB0_19:
	mov.f64 	%fd89, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd91, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd88, %fd89, %fd6, %fd91;
	// inline asm
	mov.f64 	%fd95, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd92, %fd88, %fd6, %fd95;
	// inline asm
	mov.f64 	%fd99, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd96, %fd92, %fd6, %fd99;
	// inline asm
	mov.f64 	%fd103, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd100, %fd96, %fd6, %fd103;
	// inline asm
	mov.f64 	%fd107, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd104, %fd100, %fd6, %fd107;
	// inline asm
	mul.rn.f64 	%fd109, %fd104, %fd6;
	// inline asm
	fma.rn.f64 	%fd328, %fd109, %fd327, %fd327;
	// inline asm
	bra.uni 	BB0_20;

BB0_18:
	mov.f64 	%fd61, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd63, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd60, %fd61, %fd6, %fd63;
	// inline asm
	mov.f64 	%fd67, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd64, %fd60, %fd6, %fd67;
	// inline asm
	mov.f64 	%fd71, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd68, %fd64, %fd6, %fd71;
	// inline asm
	mov.f64 	%fd75, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd72, %fd68, %fd6, %fd75;
	// inline asm
	mov.f64 	%fd79, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd76, %fd72, %fd6, %fd79;
	// inline asm
	mov.f64 	%fd83, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd80, %fd76, %fd6, %fd83;
	// inline asm
	mov.f64 	%fd87, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd328, %fd80, %fd6, %fd87;
	// inline asm

BB0_20:
	and.b32  	%r106, %r19, 2;
	setp.eq.s32	%p18, %r106, 0;
	neg.f64 	%fd112, %fd328;
	selp.f64	%fd10, %fd328, %fd112, %p18;
	mov.f64 	%fd333, %fd10;

BB0_21:
	mov.f64 	%fd11, %fd333;
	ld.param.u64 	%rd365, [map_trans_param_0];
	shl.b64 	%rd230, %rd7, 3;
	add.s64 	%rd231, %rd365, %rd230;
	ld.global.f64 	%fd12, [%rd231];
	abs.f64 	%fd13, %fd12;
	setp.eq.f64	%p19, %fd13, 0d7FF0000000000000;
	mov.f64 	%fd332, %fd44;
	@%p19 bra 	BB0_40;

	setp.gt.f64	%p20, %fd13, 0d41E0000000000000;
	@%p20 bra 	BB0_24;
	bra.uni 	BB0_23;

BB0_24:
	mov.u64 	%rd381, %rd1;
	mov.b64 	 %rd52, %fd12;
	and.b64  	%rd53, %rd52, -9223372036854775808;
	shr.u64 	%rd54, %rd52, 52;
	bfe.u64 	%rd233, %rd52, 52, 11;
	add.s64 	%rd234, %rd233, 4294966272;
	cvt.u32.u64	%r21, %rd234;
	shr.u32 	%r108, %r21, 6;
	mov.u32 	%r109, 16;
	sub.s32 	%r22, %r109, %r108;
	mov.u32 	%r110, 19;
	sub.s32 	%r111, %r110, %r108;
	mov.u32 	%r112, 18;
	min.s32 	%r23, %r112, %r111;
	setp.gt.s32	%p21, %r22, %r23;
	mov.u64 	%rd382, 0;
	@%p21 bra 	BB0_27;

	shl.b64 	%rd236, %rd52, 11;
	or.b64  	%rd55, %rd236, -9223372036854775808;
	add.s32 	%r24, %r22, -1;
	mov.u64 	%rd380, %rd1;
	cvt.u32.u64	%r113, %rd54;
	and.b32  	%r114, %r113, 2047;
	add.s32 	%r115, %r114, -1024;
	shr.u32 	%r116, %r115, 6;
	neg.s32 	%r117, %r116;
	mul.wide.s32 	%rd238, %r117, 8;
	mov.u64 	%rd239, __internal_i2opi_d;
	add.s64 	%rd240, %rd239, %rd238;
	add.s64 	%rd379, %rd240, 120;
	mov.u64 	%rd382, 0;
	mov.u32 	%r192, %r24;
	mov.u64 	%rd410, %rd1;

BB0_26:
	.pragma "nounroll";
	mov.u32 	%r25, %r192;
	ld.const.u64 	%rd241, [%rd379];
	mul.lo.s64 	%rd242, %rd241, %rd55;
	mul.hi.u64 	%rd243, %rd241, %rd55;
	add.s64 	%rd244, %rd382, %rd242;
	setp.lt.u64	%p22, %rd244, %rd382;
	selp.u64	%rd245, 1, 0, %p22;
	add.s64 	%rd382, %rd245, %rd243;
	st.local.u64 	[%rd380], %rd244;
	add.s32 	%r26, %r25, 1;
	sub.s32 	%r118, %r26, %r24;
	mul.wide.s32 	%rd246, %r118, 8;
	add.s64 	%rd380, %rd1, %rd246;
	add.s64 	%rd379, %rd379, 8;
	setp.lt.s32	%p23, %r26, %r23;
	add.s64 	%rd410, %rd410, 8;
	mov.u64 	%rd381, %rd410;
	mov.u32 	%r192, %r26;
	@%p23 bra 	BB0_26;

BB0_27:
	st.local.u64 	[%rd381], %rd382;
	ld.local.u64 	%rd383, [%rd1+24];
	ld.local.u64 	%rd384, [%rd1+16];
	and.b32  	%r119, %r21, 63;
	setp.eq.s32	%p24, %r119, 0;
	@%p24 bra 	BB0_29;

	cvt.u32.u64	%r120, %rd54;
	and.b32  	%r121, %r120, 63;
	shl.b64 	%rd247, %rd383, %r121;
	neg.s64 	%rd248, %rd54;
	cvt.u32.u64	%r122, %rd248;
	and.b32  	%r123, %r122, 63;
	shr.u64 	%rd249, %rd384, %r123;
	or.b64  	%rd383, %rd249, %rd247;
	shl.b64 	%rd250, %rd384, %r121;
	ld.local.u64 	%rd251, [%rd6];
	shr.u64 	%rd252, %rd251, %r123;
	or.b64  	%rd384, %rd252, %rd250;

BB0_29:
	shr.u64 	%rd253, %rd383, 62;
	cvt.u32.u64	%r124, %rd253;
	shr.u64 	%rd254, %rd384, 62;
	shl.b64 	%rd255, %rd383, 2;
	or.b64  	%rd389, %rd254, %rd255;
	shl.b64 	%rd76, %rd384, 2;
	setp.ne.s64	%p25, %rd76, 0;
	selp.u64	%rd256, 1, 0, %p25;
	or.b64  	%rd257, %rd256, %rd389;
	setp.gt.u64	%p26, %rd257, -9223372036854775808;
	selp.u32	%r125, 1, 0, %p26;
	add.s32 	%r27, %r125, %r124;
	setp.lt.u64	%p27, %rd257, -9223372036854775807;
	mov.u64 	%rd385, %rd53;
	mov.u64 	%rd388, %rd76;
	@%p27 bra 	BB0_31;

	not.b64 	%rd258, %rd389;
	neg.s64 	%rd77, %rd76;
	setp.eq.s64	%p28, %rd76, 0;
	selp.u64	%rd259, 1, 0, %p28;
	add.s64 	%rd389, %rd259, %rd258;
	xor.b64  	%rd79, %rd53, -9223372036854775808;
	mov.u64 	%rd385, %rd79;
	mov.u64 	%rd388, %rd77;

BB0_31:
	mov.u64 	%rd387, %rd388;
	mov.u64 	%rd81, %rd385;
	neg.s32 	%r128, %r27;
	setp.eq.s64	%p29, %rd53, 0;
	selp.b32	%r198, %r27, %r128, %p29;
	mov.u32 	%r196, 0;
	mov.u32 	%r197, %r196;
	setp.lt.s64	%p30, %rd389, 1;
	@%p30 bra 	BB0_33;

BB0_32:
	shr.u64 	%rd260, %rd387, 63;
	shl.b64 	%rd261, %rd389, 1;
	or.b64  	%rd389, %rd260, %rd261;
	shl.b64 	%rd387, %rd387, 1;
	add.s32 	%r197, %r197, -1;
	setp.gt.s64	%p31, %rd389, 0;
	mov.u32 	%r196, %r197;
	@%p31 bra 	BB0_32;

BB0_33:
	mov.u32 	%r195, %r196;
	mul.lo.s64 	%rd391, %rd389, -3958705157555305931;
	mov.u64 	%rd262, -3958705157555305931;
	mul.hi.u64 	%rd390, %rd389, %rd262;
	setp.lt.s64	%p32, %rd390, 1;
	@%p32 bra 	BB0_35;

	shl.b64 	%rd263, %rd390, 1;
	shr.u64 	%rd264, %rd391, 63;
	or.b64  	%rd390, %rd263, %rd264;
	mul.lo.s64 	%rd391, %rd389, -7917410315110611862;
	add.s32 	%r195, %r195, -1;

BB0_35:
	setp.ne.s64	%p33, %rd391, 0;
	selp.u64	%rd265, 1, 0, %p33;
	add.s64 	%rd266, %rd265, %rd390;
	add.s32 	%r129, %r195, 1022;
	cvt.u64.u32	%rd267, %r129;
	shl.b64 	%rd268, %rd267, 52;
	shr.u64 	%rd269, %rd266, 11;
	add.s64 	%rd270, %rd268, %rd269;
	bfe.u64 	%rd271, %rd266, 10, 1;
	add.s64 	%rd272, %rd270, %rd271;
	or.b64  	%rd273, %rd272, %rd81;
	mov.b64 	 %fd329, %rd273;
	bra.uni 	BB0_36;

BB0_23:
	mov.f64 	%fd127, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd114, %fd12, %fd127;
	// inline asm
	cvt.rni.s32.f64 	%r198, %fd114;
	// inline asm
	cvt.rn.f64.s32	%fd128, %r198;
	neg.f64 	%fd124, %fd128;
	mov.f64 	%fd117, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd115, %fd124, %fd117, %fd12;
	// inline asm
	mov.f64 	%fd121, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd119, %fd124, %fd121, %fd115;
	// inline asm
	mov.f64 	%fd125, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd329, %fd124, %fd125, %fd119;
	// inline asm

BB0_36:
	add.s32 	%r35, %r198, 1;
	and.b32  	%r130, %r35, 1;
	setp.eq.b32	%p34, %r130, 1;
	mul.rn.f64 	%fd17, %fd329, %fd329;
	@!%p34 bra 	BB0_38;
	bra.uni 	BB0_37;

BB0_38:
	mov.f64 	%fd158, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd160, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd157, %fd158, %fd17, %fd160;
	// inline asm
	mov.f64 	%fd164, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd161, %fd157, %fd17, %fd164;
	// inline asm
	mov.f64 	%fd168, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd165, %fd161, %fd17, %fd168;
	// inline asm
	mov.f64 	%fd172, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd169, %fd165, %fd17, %fd172;
	// inline asm
	mov.f64 	%fd176, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd173, %fd169, %fd17, %fd176;
	// inline asm
	mul.rn.f64 	%fd178, %fd173, %fd17;
	// inline asm
	fma.rn.f64 	%fd330, %fd178, %fd329, %fd329;
	// inline asm
	bra.uni 	BB0_39;

BB0_37:
	mov.f64 	%fd130, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd132, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd129, %fd130, %fd17, %fd132;
	// inline asm
	mov.f64 	%fd136, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd133, %fd129, %fd17, %fd136;
	// inline asm
	mov.f64 	%fd140, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd137, %fd133, %fd17, %fd140;
	// inline asm
	mov.f64 	%fd144, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd141, %fd137, %fd17, %fd144;
	// inline asm
	mov.f64 	%fd148, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd145, %fd141, %fd17, %fd148;
	// inline asm
	mov.f64 	%fd152, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd149, %fd145, %fd17, %fd152;
	// inline asm
	mov.f64 	%fd156, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd330, %fd149, %fd17, %fd156;
	// inline asm

BB0_39:
	and.b32  	%r131, %r35, 2;
	setp.eq.s32	%p35, %r131, 0;
	neg.f64 	%fd181, %fd330;
	selp.f64	%fd332, %fd330, %fd181, %p35;

BB0_40:
	mul.f64 	%fd182, %fd332, %fd43;
	mul.f64 	%fd183, %fd11, %fd182;
	add.s64 	%rd275, %rd182, %rd230;
	st.global.f64 	[%rd275], %fd183;
	setp.eq.f64	%p36, %fd12, 0d0000000000000000;
	or.pred  	%p38, %p19, %p36;
	@%p38 bra 	BB0_59;
	bra.uni 	BB0_41;

BB0_59:
	mov.f64 	%fd252, 0d0000000000000000;
	mul.rn.f64 	%fd336, %fd12, %fd252;
	bra.uni 	BB0_60;

BB0_41:
	setp.gt.f64	%p39, %fd13, 0d41E0000000000000;
	@%p39 bra 	BB0_43;
	bra.uni 	BB0_42;

BB0_43:
	mov.u64 	%rd394, %rd1;
	mov.b64 	 %rd95, %fd12;
	and.b64  	%rd96, %rd95, -9223372036854775808;
	shr.u64 	%rd97, %rd95, 52;
	bfe.u64 	%rd277, %rd95, 52, 11;
	add.s64 	%rd278, %rd277, 4294966272;
	cvt.u32.u64	%r37, %rd278;
	shr.u32 	%r133, %r37, 6;
	mov.u32 	%r134, 16;
	sub.s32 	%r38, %r134, %r133;
	mov.u32 	%r135, 19;
	sub.s32 	%r136, %r135, %r133;
	mov.u32 	%r137, 18;
	min.s32 	%r39, %r137, %r136;
	setp.gt.s32	%p40, %r38, %r39;
	mov.u64 	%rd395, 0;
	@%p40 bra 	BB0_46;

	shl.b64 	%rd280, %rd95, 11;
	or.b64  	%rd98, %rd280, -9223372036854775808;
	add.s32 	%r40, %r38, -1;
	mov.u64 	%rd393, %rd1;
	cvt.u32.u64	%r138, %rd97;
	and.b32  	%r139, %r138, 2047;
	add.s32 	%r140, %r139, -1024;
	shr.u32 	%r141, %r140, 6;
	neg.s32 	%r142, %r141;
	mul.wide.s32 	%rd282, %r142, 8;
	mov.u64 	%rd283, __internal_i2opi_d;
	add.s64 	%rd284, %rd283, %rd282;
	add.s64 	%rd392, %rd284, 120;
	mov.u64 	%rd395, 0;
	mov.u32 	%r199, %r40;
	mov.u64 	%rd409, %rd1;

BB0_45:
	.pragma "nounroll";
	mov.u32 	%r41, %r199;
	ld.const.u64 	%rd285, [%rd392];
	mul.lo.s64 	%rd286, %rd285, %rd98;
	mul.hi.u64 	%rd287, %rd285, %rd98;
	add.s64 	%rd288, %rd395, %rd286;
	setp.lt.u64	%p41, %rd288, %rd395;
	selp.u64	%rd289, 1, 0, %p41;
	add.s64 	%rd395, %rd289, %rd287;
	st.local.u64 	[%rd393], %rd288;
	add.s32 	%r42, %r41, 1;
	sub.s32 	%r143, %r42, %r40;
	mul.wide.s32 	%rd290, %r143, 8;
	add.s64 	%rd393, %rd1, %rd290;
	add.s64 	%rd392, %rd392, 8;
	setp.lt.s32	%p42, %r42, %r39;
	add.s64 	%rd409, %rd409, 8;
	mov.u64 	%rd394, %rd409;
	mov.u32 	%r199, %r42;
	@%p42 bra 	BB0_45;

BB0_46:
	st.local.u64 	[%rd394], %rd395;
	ld.local.u64 	%rd396, [%rd1+24];
	ld.local.u64 	%rd397, [%rd1+16];
	and.b32  	%r144, %r37, 63;
	setp.eq.s32	%p43, %r144, 0;
	@%p43 bra 	BB0_48;

	cvt.u32.u64	%r145, %rd97;
	and.b32  	%r146, %r145, 63;
	shl.b64 	%rd291, %rd396, %r146;
	neg.s64 	%rd292, %rd97;
	cvt.u32.u64	%r147, %rd292;
	and.b32  	%r148, %r147, 63;
	shr.u64 	%rd293, %rd397, %r148;
	or.b64  	%rd396, %rd293, %rd291;
	shl.b64 	%rd294, %rd397, %r146;
	ld.local.u64 	%rd295, [%rd6];
	shr.u64 	%rd296, %rd295, %r148;
	or.b64  	%rd397, %rd296, %rd294;

BB0_48:
	shr.u64 	%rd297, %rd396, 62;
	cvt.u32.u64	%r149, %rd297;
	shr.u64 	%rd298, %rd397, 62;
	shl.b64 	%rd299, %rd396, 2;
	or.b64  	%rd402, %rd298, %rd299;
	shl.b64 	%rd119, %rd397, 2;
	setp.ne.s64	%p44, %rd119, 0;
	selp.u64	%rd300, 1, 0, %p44;
	or.b64  	%rd301, %rd300, %rd402;
	setp.gt.u64	%p45, %rd301, -9223372036854775808;
	selp.u32	%r150, 1, 0, %p45;
	add.s32 	%r43, %r150, %r149;
	setp.lt.u64	%p46, %rd301, -9223372036854775807;
	mov.u64 	%rd398, %rd96;
	mov.u64 	%rd401, %rd119;
	@%p46 bra 	BB0_50;

	not.b64 	%rd302, %rd402;
	neg.s64 	%rd120, %rd119;
	setp.eq.s64	%p47, %rd119, 0;
	selp.u64	%rd303, 1, 0, %p47;
	add.s64 	%rd402, %rd303, %rd302;
	xor.b64  	%rd122, %rd96, -9223372036854775808;
	mov.u64 	%rd398, %rd122;
	mov.u64 	%rd401, %rd120;

BB0_50:
	mov.u64 	%rd400, %rd401;
	mov.u64 	%rd124, %rd398;
	neg.s32 	%r153, %r43;
	setp.eq.s64	%p48, %rd96, 0;
	selp.b32	%r205, %r43, %r153, %p48;
	mov.u32 	%r203, 0;
	mov.u32 	%r204, %r203;
	setp.lt.s64	%p49, %rd402, 1;
	@%p49 bra 	BB0_52;

BB0_51:
	shr.u64 	%rd304, %rd400, 63;
	shl.b64 	%rd305, %rd402, 1;
	or.b64  	%rd402, %rd304, %rd305;
	shl.b64 	%rd400, %rd400, 1;
	add.s32 	%r204, %r204, -1;
	setp.gt.s64	%p50, %rd402, 0;
	mov.u32 	%r203, %r204;
	@%p50 bra 	BB0_51;

BB0_52:
	mov.u32 	%r202, %r203;
	mul.lo.s64 	%rd404, %rd402, -3958705157555305931;
	mov.u64 	%rd306, -3958705157555305931;
	mul.hi.u64 	%rd403, %rd402, %rd306;
	setp.lt.s64	%p51, %rd403, 1;
	@%p51 bra 	BB0_54;

	shl.b64 	%rd307, %rd403, 1;
	shr.u64 	%rd308, %rd404, 63;
	or.b64  	%rd403, %rd307, %rd308;
	mul.lo.s64 	%rd404, %rd402, -7917410315110611862;
	add.s32 	%r202, %r202, -1;

BB0_54:
	setp.ne.s64	%p52, %rd404, 0;
	selp.u64	%rd309, 1, 0, %p52;
	add.s64 	%rd310, %rd309, %rd403;
	add.s32 	%r154, %r202, 1022;
	cvt.u64.u32	%rd311, %r154;
	shl.b64 	%rd312, %rd311, 52;
	shr.u64 	%rd313, %rd310, 11;
	add.s64 	%rd314, %rd312, %rd313;
	bfe.u64 	%rd315, %rd310, 10, 1;
	add.s64 	%rd316, %rd314, %rd315;
	or.b64  	%rd317, %rd316, %rd124;
	mov.b64 	 %fd334, %rd317;
	bra.uni 	BB0_55;

BB0_42:
	mov.f64 	%fd197, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd184, %fd12, %fd197;
	// inline asm
	cvt.rni.s32.f64 	%r205, %fd184;
	// inline asm
	cvt.rn.f64.s32	%fd198, %r205;
	neg.f64 	%fd194, %fd198;
	mov.f64 	%fd187, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd185, %fd194, %fd187, %fd12;
	// inline asm
	mov.f64 	%fd191, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd189, %fd194, %fd191, %fd185;
	// inline asm
	mov.f64 	%fd195, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd334, %fd194, %fd195, %fd189;
	// inline asm

BB0_55:
	and.b32  	%r155, %r205, 1;
	setp.eq.b32	%p53, %r155, 1;
	mul.rn.f64 	%fd26, %fd334, %fd334;
	@!%p53 bra 	BB0_57;
	bra.uni 	BB0_56;

BB0_57:
	mov.f64 	%fd228, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd230, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd227, %fd228, %fd26, %fd230;
	// inline asm
	mov.f64 	%fd234, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd231, %fd227, %fd26, %fd234;
	// inline asm
	mov.f64 	%fd238, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd235, %fd231, %fd26, %fd238;
	// inline asm
	mov.f64 	%fd242, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd239, %fd235, %fd26, %fd242;
	// inline asm
	mov.f64 	%fd246, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd243, %fd239, %fd26, %fd246;
	// inline asm
	mul.rn.f64 	%fd248, %fd243, %fd26;
	// inline asm
	fma.rn.f64 	%fd335, %fd248, %fd334, %fd334;
	// inline asm
	bra.uni 	BB0_58;

BB0_56:
	mov.f64 	%fd200, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd202, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd199, %fd200, %fd26, %fd202;
	// inline asm
	mov.f64 	%fd206, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd203, %fd199, %fd26, %fd206;
	// inline asm
	mov.f64 	%fd210, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd207, %fd203, %fd26, %fd210;
	// inline asm
	mov.f64 	%fd214, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd211, %fd207, %fd26, %fd214;
	// inline asm
	mov.f64 	%fd218, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd215, %fd211, %fd26, %fd218;
	// inline asm
	mov.f64 	%fd222, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd219, %fd215, %fd26, %fd222;
	// inline asm
	mov.f64 	%fd226, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd335, %fd219, %fd26, %fd226;
	// inline asm

BB0_58:
	and.b32  	%r156, %r205, 2;
	setp.eq.s32	%p54, %r156, 0;
	neg.f64 	%fd251, %fd335;
	selp.f64	%fd336, %fd335, %fd251, %p54;

BB0_60:
	mul.f64 	%fd253, %fd336, %fd43;
	mul.f64 	%fd254, %fd11, %fd253;
	add.s64 	%rd319, %rd183, %rd230;
	st.global.f64 	[%rd319], %fd254;
	setp.eq.f64	%p55, %fd1, 0d0000000000000000;
	or.pred  	%p57, %p2, %p55;
	@%p57 bra 	BB0_79;
	bra.uni 	BB0_61;

BB0_79:
	mov.f64 	%fd323, 0d0000000000000000;
	mul.rn.f64 	%fd339, %fd1, %fd323;
	bra.uni 	BB0_80;

BB0_61:
	abs.f64 	%fd325, %fd1;
	setp.gt.f64	%p58, %fd325, 0d41E0000000000000;
	@%p58 bra 	BB0_63;
	bra.uni 	BB0_62;

BB0_63:
	mov.u64 	%rd414, %rd1;
	mov.b64 	 %rd138, %fd1;
	and.b64  	%rd139, %rd138, -9223372036854775808;
	shr.u64 	%rd140, %rd138, 52;
	bfe.u64 	%rd321, %rd138, 52, 11;
	add.s64 	%rd322, %rd321, 4294966272;
	cvt.u32.u64	%r52, %rd322;
	shr.u32 	%r158, %r52, 6;
	mov.u32 	%r159, 16;
	sub.s32 	%r53, %r159, %r158;
	mov.u32 	%r160, 19;
	sub.s32 	%r161, %r160, %r158;
	mov.u32 	%r162, 18;
	min.s32 	%r54, %r162, %r161;
	setp.gt.s32	%p59, %r53, %r54;
	mov.u64 	%rd415, 0;
	@%p59 bra 	BB0_66;

	shl.b64 	%rd324, %rd138, 11;
	or.b64  	%rd141, %rd324, -9223372036854775808;
	add.s32 	%r55, %r53, -1;
	mov.u64 	%rd413, %rd1;
	cvt.u32.u64	%r163, %rd140;
	and.b32  	%r164, %r163, 2047;
	add.s32 	%r165, %r164, -1024;
	shr.u32 	%r166, %r165, 6;
	neg.s32 	%r167, %r166;
	mul.wide.s32 	%rd326, %r167, 8;
	mov.u64 	%rd327, __internal_i2opi_d;
	add.s64 	%rd328, %rd327, %rd326;
	add.s64 	%rd412, %rd328, 120;
	mov.u64 	%rd415, 0;
	mov.u64 	%rd408, %rd1;
	mov.u32 	%r206, %r55;

BB0_65:
	.pragma "nounroll";
	mov.u32 	%r56, %r206;
	ld.const.u64 	%rd329, [%rd412];
	mul.lo.s64 	%rd330, %rd329, %rd141;
	mul.hi.u64 	%rd331, %rd329, %rd141;
	add.s64 	%rd332, %rd415, %rd330;
	setp.lt.u64	%p60, %rd332, %rd415;
	selp.u64	%rd333, 1, 0, %p60;
	add.s64 	%rd415, %rd333, %rd331;
	st.local.u64 	[%rd413], %rd332;
	add.s32 	%r57, %r56, 1;
	sub.s32 	%r168, %r57, %r55;
	mul.wide.s32 	%rd334, %r168, 8;
	add.s64 	%rd413, %rd1, %rd334;
	add.s64 	%rd412, %rd412, 8;
	setp.lt.s32	%p61, %r57, %r54;
	add.s64 	%rd408, %rd408, 8;
	mov.u64 	%rd414, %rd408;
	mov.u32 	%r206, %r57;
	@%p61 bra 	BB0_65;

BB0_66:
	st.local.u64 	[%rd414], %rd415;
	ld.local.u64 	%rd416, [%rd1+24];
	ld.local.u64 	%rd417, [%rd1+16];
	and.b32  	%r169, %r52, 63;
	setp.eq.s32	%p62, %r169, 0;
	@%p62 bra 	BB0_68;

	cvt.u32.u64	%r170, %rd140;
	and.b32  	%r171, %r170, 63;
	shl.b64 	%rd335, %rd416, %r171;
	neg.s64 	%rd336, %rd140;
	cvt.u32.u64	%r172, %rd336;
	and.b32  	%r173, %r172, 63;
	shr.u64 	%rd337, %rd417, %r173;
	or.b64  	%rd416, %rd337, %rd335;
	shl.b64 	%rd338, %rd417, %r171;
	ld.local.u64 	%rd339, [%rd6];
	shr.u64 	%rd340, %rd339, %r173;
	or.b64  	%rd417, %rd340, %rd338;

BB0_68:
	shr.u64 	%rd341, %rd416, 62;
	cvt.u32.u64	%r174, %rd341;
	shr.u64 	%rd342, %rd417, 62;
	shl.b64 	%rd343, %rd416, 2;
	or.b64  	%rd422, %rd342, %rd343;
	shl.b64 	%rd162, %rd417, 2;
	setp.ne.s64	%p63, %rd162, 0;
	selp.u64	%rd344, 1, 0, %p63;
	or.b64  	%rd345, %rd344, %rd422;
	setp.gt.u64	%p64, %rd345, -9223372036854775808;
	selp.u32	%r175, 1, 0, %p64;
	add.s32 	%r58, %r175, %r174;
	setp.lt.u64	%p65, %rd345, -9223372036854775807;
	mov.u64 	%rd418, %rd139;
	mov.u64 	%rd421, %rd162;
	@%p65 bra 	BB0_70;

	not.b64 	%rd346, %rd422;
	neg.s64 	%rd163, %rd162;
	setp.eq.s64	%p66, %rd162, 0;
	selp.u64	%rd347, 1, 0, %p66;
	add.s64 	%rd422, %rd347, %rd346;
	xor.b64  	%rd165, %rd139, -9223372036854775808;
	mov.u64 	%rd418, %rd165;
	mov.u64 	%rd421, %rd163;

BB0_70:
	mov.u64 	%rd420, %rd421;
	mov.u64 	%rd167, %rd418;
	neg.s32 	%r178, %r58;
	setp.eq.s64	%p67, %rd139, 0;
	selp.b32	%r212, %r58, %r178, %p67;
	mov.u32 	%r210, 0;
	mov.u32 	%r211, %r210;
	setp.lt.s64	%p68, %rd422, 1;
	@%p68 bra 	BB0_72;

BB0_71:
	shr.u64 	%rd348, %rd420, 63;
	shl.b64 	%rd349, %rd422, 1;
	or.b64  	%rd422, %rd348, %rd349;
	shl.b64 	%rd420, %rd420, 1;
	add.s32 	%r211, %r211, -1;
	setp.gt.s64	%p69, %rd422, 0;
	mov.u32 	%r210, %r211;
	@%p69 bra 	BB0_71;

BB0_72:
	mov.u32 	%r209, %r210;
	mul.lo.s64 	%rd424, %rd422, -3958705157555305931;
	mov.u64 	%rd350, -3958705157555305931;
	mul.hi.u64 	%rd423, %rd422, %rd350;
	setp.lt.s64	%p70, %rd423, 1;
	@%p70 bra 	BB0_74;

	shl.b64 	%rd351, %rd423, 1;
	shr.u64 	%rd352, %rd424, 63;
	or.b64  	%rd423, %rd351, %rd352;
	mul.lo.s64 	%rd424, %rd422, -7917410315110611862;
	add.s32 	%r209, %r209, -1;

BB0_74:
	setp.ne.s64	%p71, %rd424, 0;
	selp.u64	%rd353, 1, 0, %p71;
	add.s64 	%rd354, %rd353, %rd423;
	add.s32 	%r179, %r209, 1022;
	cvt.u64.u32	%rd355, %r179;
	shl.b64 	%rd356, %rd355, 52;
	shr.u64 	%rd357, %rd354, 11;
	add.s64 	%rd358, %rd356, %rd357;
	bfe.u64 	%rd359, %rd354, 10, 1;
	add.s64 	%rd360, %rd358, %rd359;
	or.b64  	%rd361, %rd360, %rd167;
	mov.b64 	 %fd337, %rd361;
	bra.uni 	BB0_75;

BB0_62:
	mov.f64 	%fd268, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd255, %fd1, %fd268;
	// inline asm
	cvt.rni.s32.f64 	%r212, %fd255;
	// inline asm
	cvt.rn.f64.s32	%fd269, %r212;
	neg.f64 	%fd265, %fd269;
	mov.f64 	%fd258, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd256, %fd265, %fd258, %fd1;
	// inline asm
	mov.f64 	%fd262, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd260, %fd265, %fd262, %fd256;
	// inline asm
	mov.f64 	%fd266, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd337, %fd265, %fd266, %fd260;
	// inline asm

BB0_75:
	and.b32  	%r180, %r212, 1;
	setp.eq.b32	%p72, %r180, 1;
	mul.rn.f64 	%fd36, %fd337, %fd337;
	@!%p72 bra 	BB0_77;
	bra.uni 	BB0_76;

BB0_77:
	mov.f64 	%fd299, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd301, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd298, %fd299, %fd36, %fd301;
	// inline asm
	mov.f64 	%fd305, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd302, %fd298, %fd36, %fd305;
	// inline asm
	mov.f64 	%fd309, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd306, %fd302, %fd36, %fd309;
	// inline asm
	mov.f64 	%fd313, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd310, %fd306, %fd36, %fd313;
	// inline asm
	mov.f64 	%fd317, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd314, %fd310, %fd36, %fd317;
	// inline asm
	mul.rn.f64 	%fd319, %fd314, %fd36;
	// inline asm
	fma.rn.f64 	%fd338, %fd319, %fd337, %fd337;
	// inline asm
	bra.uni 	BB0_78;

BB0_76:
	mov.f64 	%fd271, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd273, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd270, %fd271, %fd36, %fd273;
	// inline asm
	mov.f64 	%fd277, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd274, %fd270, %fd36, %fd277;
	// inline asm
	mov.f64 	%fd281, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd278, %fd274, %fd36, %fd281;
	// inline asm
	mov.f64 	%fd285, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd282, %fd278, %fd36, %fd285;
	// inline asm
	mov.f64 	%fd289, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd286, %fd282, %fd36, %fd289;
	// inline asm
	mov.f64 	%fd293, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd290, %fd286, %fd36, %fd293;
	// inline asm
	mov.f64 	%fd297, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd338, %fd290, %fd36, %fd297;
	// inline asm

BB0_78:
	and.b32  	%r181, %r212, 2;
	setp.eq.s32	%p73, %r181, 0;
	neg.f64 	%fd322, %fd338;
	selp.f64	%fd339, %fd338, %fd322, %p73;

BB0_80:
	ld.param.u32 	%r182, [map_trans_param_6];
	add.s64 	%rd363, %rd184, %rd230;
	mul.f64 	%fd324, %fd339, %fd43;
	st.global.f64 	[%rd363], %fd324;
	add.s32 	%r184, %r184, 16;
	add.s32 	%r183, %r183, 1;
	setp.lt.s32	%p74, %r183, %r182;
	@%p74 bra 	BB0_2;

BB0_81:
	ret;
}


 